{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YG4jOi1Rwx2A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dani_\\anaconda3\\envs\\faceNext\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
            "  warnings.warn(\"No audio backend is available.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m[mitsuba] Warning: Couldn't import the ipywidgets package. Installing this package is required for the system to properly log messages and print in Jupyter notebooks!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "from optimizer import Optimizer\n",
        "from config import Config\n",
        "from IPython.display import Image, display\n",
        "from IPython.display import Image\n",
        "import os, glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jNPREJEczUYg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading optim config from:  ./optimConfig.ini\n"
          ]
        }
      ],
      "source": [
        "sys.path.insert(0,'/content/NextFace') #verify ur path\n",
        "\n",
        "config = Config()\n",
        "config.fillFromDicFile('./optimConfig.ini')\n",
        "# config.device = 'cuda' # torch not compiled with cuda\n",
        "config.path = './baselMorphableModel/' #verify ur path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AwYMbIY00IvC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Basel Face Model 2017 from ./baselMorphableModel//morphableModel-2017.pickle...\n",
            "loading mesh normals...\n",
            "loading uv parametrization...\n",
            "loading landmarks association file...\n",
            "creating sampler...\n",
            "loading image from path:  C:/Users/dani_/Desktop/repos/NextFace/output/Bikerman.jpg/diffuseMap_0.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dani_\\anaconda3\\envs\\faceNext\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# imagePath = './input/detailled_faces_unsplash/BikerMan.jpg'\n",
        "imagePath = './output/Bikerman.jpg/master_checkpoints/mitsuba_ref.png'\n",
        "\n",
        "outputDir = './output/' + os.path.basename(imagePath.strip('/'))\n",
        "torch.cuda.set_device(0)\n",
        "optimizer = Optimizer(outputDir ,config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5lQYBcXb90iH",
        "outputId": "4ff2113d-6eec-4b77-9d8f-4a32138ea7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading image from path:  ./output/Bikerman.jpg/master_checkpoints/mitsuba_ref.png\n",
            "detecting landmarks using: fan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dani_\\Desktop\\repos\\NextFace\\landmarksfan.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  return torch.tensor(landmarks, device = self.device)\n",
            "init camera pose...\n",
            "resuming optimization from checkpoint:  C:/Users/dani_/Desktop/repos/NextFace/output/Bikerman.jpg/master_checkpoints/stage1_output.pickle\n",
            "2/3 => Optimizing shape, statistical albedos, expression, head pose and scene light...\n",
            "  0%|          | 0/401 [00:00<?, ?it/s]c:\\Users\\dani_\\Desktop\\repos\\NextFace\\optimizer.py:426: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
            "  grad_loss = loss.grad\n",
            "  0%|          | 0/401 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "ename": "error",
          "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_operations.cpp:67: error: (-215:Assertion failed) src[i].dims <= 2 && src[i].rows == src[0].rows && src[i].type() == src[0].type() in function 'cv::hconcat'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#run the optimization now \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# optimizer.run(imagePath,doStep1=True,doStep2=True, doStep3=False)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m optimizer\u001b[39m.\u001b[39;49mrun(imagePath,checkpoint\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:/Users/dani_/Desktop/repos/NextFace/output/Bikerman.jpg/master_checkpoints/stage1_output.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m, doStep1\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,doStep2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, doStep3\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\dani_\\Desktop\\repos\\NextFace\\optimizer.py:639\u001b[0m, in \u001b[0;36mOptimizer.run\u001b[1;34m(self, imagePathOrDir, sharedIdentity, checkpoint, doStep1, doStep2, doStep3)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaveOutput(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputDir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/outputStage1\u001b[39m\u001b[39m'\u001b[39m, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstage1_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    638\u001b[0m \u001b[39mif\u001b[39;00m doStep2:\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunStep2()\n\u001b[0;32m    640\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msaveIntermediateStage:\n\u001b[0;32m    641\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaveOutput(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputDir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/outputStage2\u001b[39m\u001b[39m'\u001b[39m, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstage2_\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\dani_\\Desktop\\repos\\NextFace\\optimizer.py:445\u001b[0m, in \u001b[0;36mOptimizer.runStep2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39m# self.debugRender(rgba_img[..., 0:3],self.debugDir + '/Baseline/mitsuba/redner_ref' + str(iter))\u001b[39;00m\n\u001b[0;32m    444\u001b[0m image_grad \u001b[39m=\u001b[39m image_gradients(rgba_img)\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebugImageGrad(rgba_img[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m:\u001b[39m3\u001b[39;49m], inputTensor, image_grad[\u001b[39m0\u001b[39;49m], image_grad[\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebugDir \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/Baseline/mitsuba/mitsuba_gradient_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(\u001b[39miter\u001b[39;49m))\n\u001b[0;32m    446\u001b[0m \u001b[39m# self.debugFrameGrad(rgba_img[..., 0:3], inputTensor, grad_shapeCoeff, grad_expCoeff, grad_shCoeff, grad_rotation, grad_translation, grad_albedo,self.debugDir + '/debug_step2/mitsuba/_gradient_' + str(iter) )\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m# lightingVertexRender = self.pipeline.renderVertexBased(cameraVerts, diffAlbedo, specAlbedo, lightingOnly=True)\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m# albedoVertexRender = self.pipeline.renderVertexBased(cameraVerts, diffAlbedo, specAlbedo, albedoOnly=True)\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m# self.debugIteration(image, inputTensor,diff, albedoVertexRender, lightingVertexRender, self.debugDir + '/debug_step2/mitsuba/_' + str(iter)) # custom made\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m# also save obj\u001b[39;00m\n\u001b[0;32m    451\u001b[0m cameraNormals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline\u001b[39m.\u001b[39mmorphableModel\u001b[39m.\u001b[39mcomputeNormals(cameraVerts) \u001b[39m# only used of obj (might be too slow)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\dani_\\Desktop\\repos\\NextFace\\optimizer.py:244\u001b[0m, in \u001b[0;36mOptimizer.debugImageGrad\u001b[1;34m(self, image, target, grad_img_1, grad_img_2, outputPrefix)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdebugImageGrad\u001b[39m(\u001b[39mself\u001b[39m, image, target, grad_img_1,grad_img_2, outputPrefix):\n\u001b[0;32m    243\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m--> 244\u001b[0m         res \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mhconcat([cv2\u001b[39m.\u001b[39;49mcvtColor(image[i]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB),\n\u001b[0;32m    245\u001b[0m                             cv2\u001b[39m.\u001b[39;49mcvtColor(target[i]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB),\n\u001b[0;32m    246\u001b[0m                             cv2\u001b[39m.\u001b[39;49mcvtColor(grad_img_1[i]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY),\n\u001b[0;32m    247\u001b[0m                             cv2\u001b[39m.\u001b[39;49mcvtColor(grad_img_2[i]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)])\n\u001b[0;32m    249\u001b[0m         \u001b[39m# Concatenate vertically - combined image with textures \u001b[39;00m\n\u001b[0;32m    250\u001b[0m         debugFrame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mvconcat([np\u001b[39m.\u001b[39mpower(np\u001b[39m.\u001b[39mclip(res, \u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m), \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2.2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m])\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matrix_operations.cpp:67: error: (-215:Assertion failed) src[i].dims <= 2 && src[i].rows == src[0].rows && src[i].type() == src[0].type() in function 'cv::hconcat'\n"
          ]
        }
      ],
      "source": [
        "#run the optimization now \n",
        "# optimizer.run(imagePath,doStep1=True,doStep2=True, doStep3=False)\n",
        "optimizer.run(imagePath,checkpoint='C:/Users/dani_/Desktop/repos/NextFace/output/Bikerman.jpg/master_checkpoints/stage1_output.pickle', doStep1=False,doStep2=True, doStep3=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display results\n",
        "# Directory containing the images\n",
        "dir_path = outputDir + '/debug/Baseline/vertex_based/'\n",
        "\n",
        "# Get a list of all the .png images in the directory\n",
        "image_files = glob.glob(os.path.join(dir_path, \"*.png\"))\n",
        "\n",
        "# Display each image in turn\n",
        "for image_file in image_files:\n",
        "    print(f\"Displaying image: {os.path.basename(image_file)}\")\n",
        "    display(Image(filename=image_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display meshes in polyscope\n",
        "import polyscope as ps\n",
        "import trimesh\n",
        "import glob\n",
        "import math\n",
        "\n",
        "display_meshes = [\"vertex_based_step2_iter0.obj\", \"vertex_based_step2_iter100.obj\",\"vertex_based_step2_iter200.obj\",\"vertex_based_step2_iter300.obj\",\"vertex_based_step2_iter400.obj\"]  # replace this with your mesh names\n",
        "\n",
        "\n",
        "# Get a list of all .obj files in the directory\n",
        "obj_files = glob.glob(os.path.join(outputDir + '/debug/mesh/', \"*.obj\"))\n",
        "\n",
        "# Filter obj_files to only include the ones in display_meshes\n",
        "obj_files = [file for file in obj_files if os.path.basename(file) in display_meshes]\n",
        "\n",
        "# Define a folder where the screenshots will be saved\n",
        "screenshot_folder = outputDir+ \"/debug/screenshot_folder\"  # replace with your desired folder\n",
        "\n",
        "# Check if the folder exists, if not create it\n",
        "os.makedirs(screenshot_folder, exist_ok=True)\n",
        "\n",
        "# Initialize polyscope\n",
        "ps.init()\n",
        "# enable auto centering and scaling\n",
        "# ps.set_autocenter_structures(True)\n",
        "# ps.set_autoscale_structures(True)\n",
        "ps.set_up_dir(\"neg_y_up\")\n",
        "# ps.set_bounding_box(-1.0,2.0)\n",
        "# ps.set_SSAA_factor(4)\n",
        "# ps.set_automatically_compute_scene_extents(True)\n",
        "# ps.set_autoscale_structures(True)\n",
        "# ps.set_autocenter_structures(True)\n",
        "# ps.set_ground_plane_height_factor(1.0,True)\n",
        "ps.set_ground_plane_mode(\"none\")\n",
        "\n",
        "# # Show the mesh\n",
        "# ps.show()\n",
        "# Load and register each mesh to polyscope\n",
        "for idx, obj_file in enumerate(obj_files):\n",
        "    # mesh_name = os.path.basename(obj_file)  # get the name of the mesh\n",
        "    mesh_name = os.path.basename(obj_file)  # get the name of the mesh\n",
        "\n",
        "    # Load the mesh from an obj file using trimesh\n",
        "    mesh = trimesh.load_mesh(obj_file)\n",
        "    # Register the mesh to polyscope\n",
        "    ps_mesh = ps.register_surface_mesh(mesh_name, mesh.vertices, mesh.faces)\n",
        "    # Reset the transformation on the mesh\n",
        "    ps_mesh.reset_transform()\n",
        "    # Set a color for the mesh\n",
        "    color = torch.tensor([0.5, 0.5, 0.5]).numpy()  # set a fixed color\n",
        "    ps_mesh.set_color(color)\n",
        "    # Reset the view\n",
        "    ps.reset_camera_to_home_view()\n",
        "    # Show the mesh\n",
        "    ps.show()\n",
        "\n",
        "    # Take a screenshot and save it to the specified folder\n",
        "    screenshot_name = os.path.join(screenshot_folder, f\"{mesh_name}_screenshot.png\")\n",
        "    ps.screenshot(screenshot_name)\n",
        "\n",
        "    # Remove the mesh from polyscope to prepare for the next one\n",
        "    ps.remove_all_structures()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#display results\n",
        "# Directory containing the images\n",
        "dir_path = outputDir + '/debug/screenshot_folder/'\n",
        "\n",
        "# Get a list of all the .png images in the directory\n",
        "image_files = glob.glob(os.path.join(dir_path, \"*.png\"))\n",
        "\n",
        "# Display each image in turn\n",
        "for image_file in image_files:\n",
        "    print(f\"Displaying image: {os.path.basename(image_file)}\")\n",
        "    display(Image(filename=image_file))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
