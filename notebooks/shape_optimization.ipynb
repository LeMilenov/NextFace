{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape optimization\n",
    "=================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "In this tutorial, we will optimize a triangle mesh to match a target shape specfied using a set of reference renderings.\n",
    "\n",
    "Gradients with regards to vertex positions are typically extremely sparse, since only vertices located on visibility discontinuities receive a contribution. As a consequence, naively optimizing a triangle mesh generally results in horrible, tangled meshes.\n",
    "\n",
    "To avoid this, we will use the method from the paper \"[Large Steps in Inverse Rendering of Geometry][1]\". This method optimizes a latent variable that enables smoother gradients.\n",
    "\n",
    "<div class=\"admonition important alert alert-block alert-success\">\n",
    "\n",
    "ðŸš€ **You will learn how to:**\n",
    "    \n",
    "<ul>\n",
    "  <li>Create a sensor that batches several viewpoints together</li>\n",
    "  <li>Use the \"large steps\" algorithm to optimize a shape</li>\n",
    "  <li>Use remeshing to refine the optimized shape</li>\n",
    "</ul>\n",
    "    \n",
    "</div>\n",
    "\n",
    "[1]: http://rgl.epfl.ch/publications/Nicolet2021Large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, let's import `drjit` and `mitsuba` and set a differentiation-aware variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "\n",
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "import torch\n",
    "\n",
    "\n",
    "mi.set_variant('cuda_ad_rgb', 'llvm_ad_rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'drjit' has no attribute 'set_num_threads'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m dr\u001b[39m.\u001b[39;49mset_num_threads(\u001b[39m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m mi\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mdr\u001b[39m.\u001b[39mset_device(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'drjit' has no attribute 'set_num_threads'"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "mi.util.dr.set_device(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched rendering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accurately recover a shape, we need several reference renderings, taken from different viewpoints. Rather than rendering each viewpoint separately during the optimisation like in the [volume optimisation tutorial][1], we will use the [<code>batch</code>][2] sensor, that allows us to render all views at once. There is a performance benefit to this approach: the just-in-time compiler only needs to trace the rendering process once instead of multiple times (once for each viewpoint).\n",
    "\n",
    "Note that we also have to set the `sample_border` flag to `True` for the [<code>hdrfilm</code>][3] plugin. By enabling this option, Mitsuba will render regions slightly beyond the film's boundaries, enabling us to track objects that enter or exit the frame.\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/volume_optimization.html#Optimization\n",
    "[2]: https://mitsuba.readthedocs.io/en/stable/src/generated/plugins_sensors.html#batch-sensor-batch\n",
    "[3]: https://mitsuba.readthedocs.io/en/stable/src/generated/plugins_films.html#high-dynamic-range-film-hdrfilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_count = 8\n",
    "sensor = {\n",
    "    'type': 'batch',\n",
    "    'film': {\n",
    "        'type': 'hdrfilm',\n",
    "        'width': 256 * sensor_count, 'height': 256,\n",
    "        'filter': {'type': 'gaussian'},\n",
    "        'sample_border': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch sensor expects a list of nested sensors. Here we will generate 8 viewpoints evenly distributed on the sphere, using the [Fibonacci lattice][1].\n",
    "\n",
    "[1]: http://extremelearning.com.au/evenly-distributing-points-on-a-sphere/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitsuba import ScalarTransform4f as T\n",
    "\n",
    "golden_ratio = (1 + 5**0.5)/2\n",
    "for i in range(sensor_count):\n",
    "    theta = 2 * dr.pi * i / golden_ratio\n",
    "    phi = dr.acos(1 - 2*(i+0.5)/sensor_count)\n",
    "    \n",
    "    d = 5\n",
    "    origin = [\n",
    "        d * dr.cos(theta) * dr.sin(phi),\n",
    "        d * dr.sin(theta) * dr.sin(phi),\n",
    "        d * dr.cos(phi)\n",
    "    ]\n",
    "    \n",
    "    sensor[f\"sensor_{i}\"] = {\n",
    "        'type': 'perspective',\n",
    "        'fov': 45,\n",
    "        'to_world': T.look_at(target=[0, 0, 0], origin=origin, up=[0, 1, 0])\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene construction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate the reference renderings. We will load a scene with the previous sensor, the target mesh, and an environment map. Note the use of the `direct_reparam` integrator, that properly accounts for visibility discontinuities when differentiating, as in the [object pose estimation tutorial][1].\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/object_pose_estimation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict = {\n",
    "    'type': 'scene',\n",
    "    'integrator': {\n",
    "        'type': 'direct_reparam',\n",
    "    },\n",
    "    'sensor': sensor,\n",
    "    'emitter': {\n",
    "        'type': 'envmap',\n",
    "        'filename': \"../scenes/textures/envmap2.exr\",\n",
    "    },\n",
    "    'shape': {\n",
    "        'type': 'ply',\n",
    "        'filename': \"../scenes/meshes/suzanne.ply\",\n",
    "        'bsdf': {'type': 'diffuse'}\n",
    "    }\n",
    "}\n",
    "\n",
    "scene_target = mi.load_dict(scene_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the reference image, our goal is to reconstruct [Blender's Suzanne][1].\n",
    "When using the [<code>batch</code>][1] sensor, the output is a single image of all viewpoints stitched together horizontally.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Blender_(software)\n",
    "[2]: https://mitsuba.readthedocs.io/en/stable/src/generated/plugins_sensors.html#batch-sensor-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_output(out: mi.TensorXf):\n",
    "    fig, ax = plt.subplots(figsize=(5*sensor_count, 5))\n",
    "    ax.imshow(mi.util.convert_to_bitmap(out))\n",
    "    ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img = mi.render(scene_target, spp=256)\n",
    "plot_batch_output(ref_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting geometry which we'll optimize is a relatively dense sphere. More challenging scenes and target shapes might require a better initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict['shape']['filename'] = '../scenes/meshes/ico_10k.ply'\n",
    "scene_source = mi.load_dict(scene_dict)\n",
    "\n",
    "init_img = mi.render(scene_source, spp=128)\n",
    "plot_batch_output(init_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to apply the same steps as in previous tutorials, i.e. simply render the scene, compute a loss, and differentiate. Let's try that first.\n",
    "\n",
    "As per usual, all that is needed is an `Optimizer` and a simple optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = mi.traverse(scene_source)\n",
    "opt = mi.ad.Adam(lr=1e-1)\n",
    "opt['shape.vertex_positions'] = params['shape.vertex_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(5):\n",
    "    loss = mi.Float(0.0)\n",
    "\n",
    "    params.update(opt)\n",
    "\n",
    "    img = mi.render(scene_source, params, seed=it, spp=16)\n",
    "\n",
    "    # L1 Loss\n",
    "    loss = dr.mean(dr.abs(img - ref_img))\n",
    "    dr.backward(loss)\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Iteration {1+it:03d}: Loss = {loss[0]:6f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = mi.render(scene_source, spp=128)\n",
    "plot_batch_output(final_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even after only a few steps, this approach produces an unusable mess of triangles. This is a consequence of the sparsity of the visibility-related gradients. Indeed, these are only present on edges that are on the silouhette of the mesh for a given viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the scene\n",
    "scene_source = mi.load_dict(scene_dict)\n",
    "params = mi.traverse(scene_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Steps Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than directly optimizing cartesian vertex coordinates, the work of [[Nicolet et al. 2021]][1] \"_Large Steps in Inverse Rendering of Geometry_\" introduces a latent representation that improves the smoothnes of gradients. In short, a latent variable $u$ is defined as $u = (I + \\lambda L) v$, where $v$ denotes the vertex positions, $L$ is the (combinatorial) Laplacian of the given mesh, and $\\lambda$ is a user-defined hyper-parameter. Intuitively, this parameter $\\lambda$ defines by how much the gradients should be smoothed out on the surface.\n",
    "\n",
    "This approach is readily available in Mitsuba in the `mi.ad.LargeSteps` class. It requires [<code>cholespy</code>][1], a Python package to solve sparse linear systems with Cholesky factorisations.\n",
    "\n",
    "[1]: http://rgl.epfl.ch/publications/Nicolet2021Large\n",
    "[2]: https://github.com/rgl-epfl/cholespy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cholespy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LargeSteps` helper is instanciated from the starting shapes' vertex positions and faces. If the mesh has duplicate vertices (e.g. due to face normals or UV seams), it will internally convert the mesh to a \"unique\" representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 25\n",
    "ls = mi.ad.LargeSteps(params['shape.vertex_positions'], params['shape.faces'], lambda_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use a slighlty modified version of the [<code>Adam</code>][1] optimizer, that uses a uniform second moment for all parameters. This can be done by setting `uniform=True` when instantiating the optimizer.\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/stable/src/api_reference.html#mitsuba.ad.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = mi.ad.Adam(lr=1e-1, uniform=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LargeSteps` class has a couple utility methods that can be used to convert back and forth between cartesian and differential representation of the vertex coordinates. The `LargeSteps.to_differential` method is used here to initialize the latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['u'] = ls.to_differential(params['shape.vertex_positions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation loop must also be slighlty changed. We now need to update the shape using the latent variable, this additional step can be done by using `LargeSteps.from_differential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100 if 'PYTEST_CURRENT_TEST' not in os.environ else 5\n",
    "for it in range(iterations):\n",
    "    loss = mi.Float(0.0)\n",
    "\n",
    "    # Retrieve the vertex positions from the latent variable\n",
    "    params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "    params.update()\n",
    "\n",
    "    img = mi.render(scene_source, params, seed=it, spp=16)\n",
    "\n",
    "    # L1 Loss\n",
    "    loss = dr.mean(dr.abs(img - ref_img))\n",
    "    dr.backward(loss)\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Iteration {1+it:03d}: Loss = {loss[0]:6f}\", end='\\r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the mesh after the last iteration's gradient step\n",
    "params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "params.update();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again plot the result of our optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = mi.render(scene_source, spp=128)\n",
    "plot_batch_output(final_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remeshing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above can be further improved with the help of remeshing. By increasing the tesselation of the mesh, we will be able to recover more details of the target shape. Intuitively, the intent of this step is similar to other \"coarse-to-fine\" optimization strategies. For example, in the [caustics][1] or the [volume optimization][2] tutorial we increase the resolution of texture that is being optimized over time.\n",
    "\n",
    "We will use the Botsch-Kobbelt remeshing algorithm provided by the `gpytoolbox` package:\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/caustics_optimization.html\n",
    "[2]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/volume_optimization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpytoolbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `gpytoolbox` package expects NumPy arrays, we will first convert the mesh data to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "v_np = params['shape.vertex_positions'].numpy().reshape((-1,3)).astype(np.float64)\n",
    "f_np = params['shape.faces'].numpy().reshape((-1,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Botsch-Kobbelt remeshing algorithm takes a \"target edge length\" as input argument. This controls the desired tesselation of the mesh. Since we want to increase resolution, we will set this as half of the mean edge length of the current mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average edge length\n",
    "l0 = np.linalg.norm(v_np[f_np[:,0]] - v_np[f_np[:,1]], axis=1)\n",
    "l1 = np.linalg.norm(v_np[f_np[:,1]] - v_np[f_np[:,2]], axis=1)\n",
    "l2 = np.linalg.norm(v_np[f_np[:,2]] - v_np[f_np[:,0]], axis=1)\n",
    "\n",
    "target_l = np.mean([l0, l1, l2]) / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the Botsch-Kobbelt remeshing algorithm. It runs for a user-specified number of iterations, which we set to 5 here. Further details on about this algorithm, can be found it the package's [documentation][1].\n",
    "\n",
    "[1]: https://gpytoolbox.org/0.1.0/remesh_botsch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytoolbox import remesh_botsch\n",
    "\n",
    "v_new, f_new = remesh_botsch(v_np, f_np, i=5, h=target_l, project=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new vertices and faces must now be passed to our Mitsuba `Mesh`. If the mesh has other attributes (e.g. UV coordinates), they also need to be updated. By default, Mitsuba will reset these to 0 if the vertex or face count is altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['shape.vertex_positions'] = mi.Float(v_new.flatten().astype(np.float32))\n",
    "params['shape.faces'] = mi.Int(f_new.flatten())\n",
    "params.update();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mesh topology has changed, we also need to compute a new latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = mi.ad.LargeSteps(params['shape.vertex_positions'], params['shape.faces'], lambda_)\n",
    "opt = mi.ad.Adam(lr=1e-1, uniform=True)\n",
    "opt['u'] = ls.to_differential(params['shape.vertex_positions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 150 if 'PYTEST_CURRENT_TEST' not in os.environ else 5\n",
    "for it in range(iterations):\n",
    "    loss = mi.Float(0.0)\n",
    "\n",
    "    # Retrieve the vertex positions from the latent variable\n",
    "    params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "    params.update()\n",
    "\n",
    "    img = mi.render(scene_source, params, seed=it, spp=16)\n",
    "\n",
    "    # L1 Loss\n",
    "    loss = dr.mean(dr.abs(img - ref_img))\n",
    "    dr.backward(loss)\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Iteration {1+it:03d}: Loss = {loss[0]:6f}\", end='\\r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover the final state from the latent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "params.update();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare our end result (bottom) to our reference views (top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx-thumbnail": {}
   },
   "outputs": [],
   "source": [
    "final_img = mi.render(scene_source, spp=128)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(5*sensor_count, 10))\n",
    "ax[0].set_title(\"Reference\", y=0.3, x=-0.005, rotation=90, fontsize=20)\n",
    "ax[1].set_title(\"Optimized shape\", y=0.2, x=-0.005, rotation=90, fontsize=20)\n",
    "for i, img in enumerate((ref_img, final_img)):\n",
    "    ax[i].imshow(mi.util.convert_to_bitmap(img))\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results could be further improved by e.g. using more input views, or using a less agressive step size and more iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "- [<code>mitsuba.ad.LargeSteps</code>][1]\n",
    "- [<code>direct_reparam</code> plugin][2]\n",
    "- [<code>batch</code> plugin][3]\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.ad.LargeSteps\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_integrators.html#reparameterized-direct-integrator-direct-reparam\n",
    "[3]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_sensors.html#batch-sensor-batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83642eaf50c97d4e19d0a23d915e5d4e870af428ff693683146158fe3feeea5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
