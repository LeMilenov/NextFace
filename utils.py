import numpy as np
import drjit as dr
import torch
import cv2
import os

def saveObj(filename, materialName, vertices, faces, normals = None, tcoords = None, textureFileName = 'texture.png'):
    '''
    write mesh to an obj file
    :param filename: path to where to save the obj file
    :param materialFileName: material name
    :param vertices:  float tensor [n, 3]
    :param faces: tensor [#triangles, 3]
    :param normals: float tensor [n, 3]
    :param tcoords: float tensor [n, 2]
    :param textureFileName: name of the texture to use with material
    :return:
    '''
    assert(vertices.dim() == 2 and  vertices.shape[-1] == 3)
    assert (faces.dim() == 2 and faces.shape[-1] == 3)

    if normals is not None:
        assert (normals.dim() == 2 and normals.shape[-1] == 3)

    if tcoords is not None:
        assert (tcoords.dim() == 2 and tcoords.shape[-1] == 2)

    if torch.is_tensor(vertices):
        vertices = vertices.detach().cpu().numpy()
    if torch.is_tensor(faces):
        faces = faces.detach().cpu().numpy()
    if torch.is_tensor(normals):
        normals = normals.detach().cpu().numpy()
    if torch.is_tensor(tcoords):
        tcoords = tcoords.detach().cpu().numpy()

    assert(isinstance(vertices, np.ndarray))
    assert (isinstance(faces, np.ndarray))
    assert (isinstance(normals, np.ndarray))
    assert (isinstance(tcoords, np.ndarray))

    #write material
    f = open(os.path.dirname(filename) + '/' + materialName, 'w')
    f.write('newmtl material0\n')
    f.write('map_Kd ' + textureFileName + '\n')
    f.close()

    f = open(filename, 'w')
    f.write('###########################################################\n')
    f.write('# OBJ file generated by faceYard 2021\n')
    f.write('#\n')
    f.write('# Num Vertices: %d\n' % (vertices.shape[0]))
    f.write('# Num Triangles: %d\n' % (faces.shape[0]))
    f.write('#\n')
    f.write('###########################################################\n')
    f.write('\n')
    f.write('mtllib ' + materialName + '\n')

    #write vertices
    for v in vertices:
        f.write('v %f %f %f\n' % (v[0], v[1], v[2]))

    # write the tcoords
    if tcoords is not None and tcoords.shape[0] > 0:
        for uv in tcoords:
            f.write('vt %f %f\n' % (uv[0], uv[1]))

    #write the normals
    if normals is not None and normals.shape[0] > 0:
        for n in normals:
            f.write('vn %f %f %f\n' % (n[0], n[1], n[2]))

    f.write('usemtl material0\n')
    #write face indices list
    for t in faces:
        f.write('f %d/%d/%d %d/%d/%d %d/%d/%d\n' % (t[0] + 1, t[0] + 1,t[0] + 1,
                                              t[1] + 1, t[1] + 1,t[1] + 1,
                                              t[2] + 1, t[2] + 1, t[2] + 1))
    f.close()
def saveLandmarksVerticesProjections(imageTensor, projPoints, landmarks):
    '''
    for debug, render the projected vertices and landmakrs on image
    :param images: [w, h, 3]
    :param projPoints: [n, 3]
    :param landmarks: [n, 2]
    :return: tensor [w, h, 3
    '''
    assert(imageTensor.dim() == 3 and imageTensor.shape[-1] == 3 )
    assert(projPoints.dim() == 2 and projPoints.shape[-1] == 2)
    assert(projPoints.shape == landmarks.shape)
    image = imageTensor.clone().detach().cpu().numpy() * 255.
    landmarkCount = landmarks.shape[0]
    for i in range(landmarkCount):
        x = landmarks[i, 0]
        y = landmarks[i, 1]
        cv2.circle(image, (int(x), int(y)), 2, (0, 255, 0), -1)
        x = projPoints[i, 0]
        y = projPoints[i, 1]
        cv2.circle(image, (int(x), int(y)), 2, (0, 0, 255), -1)

    return image

def mkdir_p(path):
    import errno
    import os

    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise
def loadDictionaryFromPickle(picklePath):
    import pickle
    handle = open(picklePath, 'rb')
    assert handle is not None
    dic = pickle.load(handle)
    handle.close()
    return dic
def writeDictionaryToPickle(dict, picklePath):
    import pickle
    handle = open(picklePath, 'wb')
    pickle.dump(dict, handle, pickle.HIGHEST_PROTOCOL)
    handle.close()
    
# CODE from 
# https://github.com/mitsuba-renderer/mitsuba3/issues/171
# https://github.com/mitsuba-renderer/drjit/commit/d7a6fffba1ff1ae381dda9a7ff98388214088137

def to_torch(arg):
    '''
    Convert a differentiable Dr.Jit array into a PyTorch tensor while keeping
    track of derivative computation.
    Using this function is it possible to mix AD-aware computation in Dr.Jit
    and PyTorch. As shown in the code example below, a differentiable array
    (tracking derivatives) resulting from some Dr.Jit arithmetic can be converted
    into a PyTorch tensor to perform further computation. A subsequent call to
    `torch.tensor.backward() <https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html>`_
    will properly backpropagate gradients across the frameworks, pushing
    gradients all the way to the original Dr.Jit array.
    .. code-block:: python
        # Start with a Dr.Jit array
        a = dr.llvm.ad.Float(...)
        dr.enable_grad(a)
        # Some Dr.Jit arithmetic
        b = dr.sin(a)
        # Convert it into a PyTorch tensor
        c = dr.to_torch(b)
        # Some PyTorch arithmetic
        d = c.sum()
        # Propagate gradients to variable a
        d.backward()
        # Inspect the resulting gradients
        print(dr.grad(a))
    This function relies on the `torch.autograd.Function <https://pytorch.org/docs/stable/notes/extending.html>`_
    class to implement the conversion as a custom differentiable operation in PyTorch.
    .. danger::
        Forward-mode AD isn't currently supported by this operation.
    Args:
        arg (drjit.ArrayBase): differentiable Dr.Jit array or tensor type.
    Returns:
        torch.tensor: The PyTorch tensor representing the input Dr.Jit array.
    '''
    import torch
    import torch.autograd

    class ToTorch(torch.autograd.Function):
        @staticmethod
        def forward(ctx, arg, handle):
            ctx.drjit_arg = arg
            return torch.tensor(np.array(arg))

        @staticmethod
        @torch.autograd.function.once_differentiable
        def backward(ctx, grad_output):
            dr.set_grad(ctx.drjit_arg, grad_output)
            dr.enqueue(dr.ADMode.Backward, ctx.drjit_arg)
            dr.traverse(type(ctx.drjit_arg), dr.ADMode.Backward)
            del ctx.drjit_arg
            return None, None


    handle = torch.empty(0, requires_grad=True)
    return ToTorch.apply(arg, handle)


def from_torch(dtype, arg):
    '''
    Convert a differentiable PyTorch tensor into a Dr.Jit array while keeping
    track of derivative computation.
    Using this function is it possible to mix AD-aware computation in Dr.Jit
    and PyTorch. As shown in the code example below, an differentiable tensor
    (tracking derivatives) resulting from some PyTorch arithmetic can be converted
    into a Dr.Jit array to perform further computation. A subsequent call to
    :py:func:`drjit.backward()` will properly backpropagate gradients across the
    frameworks, pushing gradients all the way to the original PyTorch tensor.
    .. code-block:: python
        # Start with a PyTorch tensor
        a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
        # Some PyTorch arithmetic
        b = torch.sin(a)
        # Convert it into a Dr.Jit array
        c = dr.from_torch(m.Float, b)
        # Some Dr.Jit arithmetic
        e = dr.sum(d)
        # Propagate gradients to variable a
        dr.backward(e)
        # Inspect the resulting gradients
        print(a.grad)
    This function relies on the :py:class:`CustomOp` class to implement the
    conversion as a custom differentiable operation in Dr.Jit.
    .. danger::
        Forward-mode AD isn't currently supported by this operation.
    Args:
        dtype (type): Desired differentiable Dr.Jit array type.
        arg (torch.tensor): PyTorch tensor type
    Returns:
        drjit.ArrayBase: The differentiable Dr.Jit array representing the input
                        PyTorch tensor.
    '''
    import torch
    if not dr.is_diff_v(dtype) or not dr.is_array_v(dtype):
        raise TypeError("from_torch(): expected a differentiable Dr.Jit array type!")

    class FromTorch(dr.CustomOp):
        def eval(self, arg, handle):
            self.torch_arg = arg
            return dtype(arg)

        def forward(self):
            raise TypeError("from_torch(): forward-mode AD is not supported!")

        def backward(self):
            grad = torch.tensor(np.array(self.grad_out()))
            self.torch_arg.backward(grad)

    handle = dr.zeros(dtype)
    dr.enable_grad(handle)
    return dr.custom(FromTorch, arg, handle)