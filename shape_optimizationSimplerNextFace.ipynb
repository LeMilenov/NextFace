{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape optimization\n",
    "=================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As always, let's import `drjit` and `mitsuba` and set a differentiation-aware variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "import torch\n",
    "\n",
    "mi.set_variant('cuda_ad_rgb', 'llvm_ad_rgb')\n",
    "torch.cuda.set_device(0)\n",
    "mi.util.dr.set_device(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitsuba import ScalarTransform4f as T\n",
    "\n",
    "scene_dict = {\n",
    "    'type': 'scene',\n",
    "    'integrator': {\n",
    "        'type': 'direct_reparam',\n",
    "    },\n",
    "    'sensor': {\n",
    "        'type': 'perspective',\n",
    "        'fov': 4.8,\n",
    "        'to_world': T.look_at(target=[0, 0, 1], origin=[0,0,0], up=[0, -1, 0]),\n",
    "        'film': {\n",
    "            'type': 'hdrfilm',\n",
    "            'width': 256,\n",
    "            'height': 256,\n",
    "            'filter': {'type': 'gaussian'},\n",
    "            'sample_border': True\n",
    "        }\n",
    "    },\n",
    "    'emitter': {\n",
    "        'type': 'envmap',\n",
    "        'filename': \"./output/redner-bikerman/envMap_0.png\",\n",
    "    },\n",
    "    'shape': {\n",
    "        'type': 'obj',\n",
    "        'filename': \"./output/Bikerman.jpg/debug/mesh/debug1_iter100.obj\", # already optimised from step 1, already in camera verts\n",
    "        'bsdf': {'type': 'diffuse'}\n",
    "    }\n",
    "}\n",
    "\n",
    "scene_target = mi.load_dict(scene_dict)\n",
    "# generate pictures from all view points\n",
    "def plot_batch_output(out: mi.TensorXf):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(mi.util.convert_to_bitmap(out))\n",
    "    ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img = mi.render(scene_target, spp=256)\n",
    "plot_batch_output(ref_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ref image from file\n",
    "from image import Image, ImageFolder, overlayImage, saveImage\n",
    "inputImage = Image('./input/detailled_faces_unsplash/BikerManCropped.jpg', 'cuda', 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_source = mi.load_dict(scene_dict)\n",
    "\n",
    "img = mi.render(scene_source, spp=256)\n",
    "plot_batch_output(img)\n",
    "# input tensor pretty much\n",
    "input_img = inputTensor = torch.pow(inputImage.tensor, inputImage.gamma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Steps Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cholespy\n",
    "lambda_ = 25\n",
    "params = mi.traverse(scene_source)\n",
    "\n",
    "ls = mi.ad.LargeSteps(params['shape.vertex_positions'], params['shape.faces'], lambda_)\n",
    "# We also use a slighlty modified version of the adam optimizer, that uses a uniform second moment for all parameters. This can be done by setting `uniform=True` when instantiating the optimizer.\n",
    "opt = mi.ad.Adam(lr=1e-1, uniform=True)\n",
    "# convert back and forth cartesian -> diff representation vertex coords.\n",
    "# init latent variable\n",
    "opt['u'] = ls.to_differential(params['shape.vertex_positions'])\n",
    "\n",
    "#otpimisation \n",
    "iterations = 500 if 'PYTEST_CURRENT_TEST' not in os.environ else 5\n",
    "for it in range(iterations):\n",
    "    loss = mi.Float(0.0)\n",
    "\n",
    "    # Retrieve the vertex positions from the latent variable\n",
    "    params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "    params.update()\n",
    "\n",
    "    current_img = mi.render(scene_source, params, seed=it, spp=32)\n",
    "\n",
    "    # L1 Loss\n",
    "    loss = dr.mean(dr.abs(current_img - mi.TensorXf(input_img.squeeze(0))))\n",
    "    dr.backward(loss)\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"Iteration {1+it:03d}: Loss = {loss[0]:6f}\", end='\\r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the mesh after the last iteration's gradient step\n",
    "params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "params.update();\n",
    "\n",
    "final_img = mi.render(scene_source, spp=128)\n",
    "plot_batch_output(ref_img) # start\n",
    "plot_batch_output(mi.TensorXf(input_img.squeeze(0))) # input\n",
    "plot_batch_output(final_img) # final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remeshing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above can be further improved with the help of remeshing. By increasing the tesselation of the mesh, we will be able to recover more details of the target shape. Intuitively, the intent of this step is similar to other \"coarse-to-fine\" optimization strategies. For example, in the [caustics][1] or the [volume optimization][2] tutorial we increase the resolution of texture that is being optimized over time.\n",
    "\n",
    "We will use the Botsch-Kobbelt remeshing algorithm provided by the `gpytoolbox` package:\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/caustics_optimization.html\n",
    "[2]: https://mitsuba.readthedocs.io/en/stable/src/inverse_rendering/volume_optimization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gpytoolbox\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "v_np = params['shape.vertex_positions'].numpy().reshape((-1,3)).astype(np.float64)\n",
    "f_np = params['shape.faces'].numpy().reshape((-1,3))\n",
    "# The Botsch-Kobbelt remeshing algorithm takes a \"target edge length\" as input argument. This controls the desired tesselation of the mesh. \n",
    "# Since we want to increase resolution, we will set this as half of the mean edge length of the current mesh.\n",
    "# Compute average edge length\n",
    "l0 = np.linalg.norm(v_np[f_np[:,0]] - v_np[f_np[:,1]], axis=1)\n",
    "l1 = np.linalg.norm(v_np[f_np[:,1]] - v_np[f_np[:,2]], axis=1)\n",
    "l2 = np.linalg.norm(v_np[f_np[:,2]] - v_np[f_np[:,0]], axis=1)\n",
    "\n",
    "target_l = np.mean([l0, l1, l2]) / 2\n",
    "\n",
    "#We can now run the Botsch-Kobbelt remeshing algorithm. It runs for a user-specified number of iterations, which we set to 5 here. \n",
    "# Further details on about this algorithm, can be found it the package's [documentation][1].\n",
    "from gpytoolbox import remesh_botsch\n",
    "v_new, f_new = remesh_botsch(v_np, f_np, i=5, h=target_l, project=True)\n",
    "\n",
    "# put v_new and f_new on gpu again\n",
    "# Create torch tensors from the numpy arrays\n",
    "v_new_t = torch.from_numpy(v_new).float().cuda()\n",
    "f_new_t = torch.from_numpy(f_new).int().cuda()\n",
    "\n",
    "# # Move tensors to the GPU\n",
    "v_new_t = v_new_t.to('cuda')\n",
    "f_new_t = f_new_t.to('cuda')\n",
    "\n",
    "# The new vertices and faces must now be passed to our Mitsuba `Mesh`. If the mesh has other attributes (e.g. UV coordinates), they also need to be updated. By default, Mitsuba will reset these to 0 if the vertex or face count is altered.\n",
    "params = mi.traverse(scene_source)\n",
    "# params['shape.vertex_positions'] = mi.Float(v_new.flatten().astype(np.float32))\n",
    "# params['shape.faces'] = mi.Int(f_new.flatten())\n",
    "params['shape.vertex_positions'] = dr.ravel(mi.TensorXf(v_new_t.to(torch.float32)))\n",
    "params['shape.faces'] = dr.ravel(mi.TensorXf(f_new_t.to(torch.float32)))\n",
    "params.update();\n",
    "\n",
    "# mesh topology is different, we need new latent variable\n",
    "ls = mi.ad.LargeSteps(params['shape.vertex_positions'], params['shape.faces'], lambda_)\n",
    "opt = mi.ad.Adam(lr=1e-1, uniform=True)\n",
    "opt['u'] = ls.to_differential(params['shape.vertex_positions'])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# optimisation\n",
    "# iterations = 150 if 'PYTEST_CURRENT_TEST' not in os.environ else 5\n",
    "# for it in range(iterations):\n",
    "#     loss = mi.Float(0.0)\n",
    "\n",
    "#     # Retrieve the vertex positions from the latent variable\n",
    "#     params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "#     params.update()\n",
    "\n",
    "#     current_img = mi.render(scene_source, params, seed=it, spp=16)\n",
    "\n",
    "#     # L1 Loss\n",
    "#     loss = dr.mean(dr.abs(current_img - mi.TensorXf(input_img.squeeze(0))))\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     dr.backward(loss)\n",
    "#     opt.step()\n",
    "\n",
    "#     print(f\"Iteration {1+it:03d}: Loss = {loss[0]:6f}\", end='/r')\n",
    "    \n",
    "params['shape.vertex_positions'] = ls.from_differential(opt['u'])\n",
    "params.update();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare our end result (bottom) to our reference views (top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx-thumbnail": {}
   },
   "outputs": [],
   "source": [
    "final_img = mi.render(scene_source, spp=128)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(5, 10))\n",
    "ax[0].set_title(\"Reference\", y=0.3, x=-0.005, rotation=90, fontsize=20)\n",
    "ax[1].set_title(\"Optimized shape\", y=0.2, x=-0.005, rotation=90, fontsize=20)\n",
    "for i, img in enumerate((ref_img, final_img)):\n",
    "    ax[i].imshow(mi.util.convert_to_bitmap(img))\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results could be further improved by e.g. using more input views, or using a less agressive step size and more iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "- [<code>mitsuba.ad.LargeSteps</code>][1]\n",
    "- [<code>direct_reparam</code> plugin][2]\n",
    "- [<code>batch</code> plugin][3]\n",
    "\n",
    "[1]: https://mitsuba.readthedocs.io/en/latest/src/api_reference.html#mitsuba.ad.LargeSteps\n",
    "[2]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_integrators.html#reparameterized-direct-integrator-direct-reparam\n",
    "[3]: https://mitsuba.readthedocs.io/en/latest/src/generated/plugins_sensors.html#batch-sensor-batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83642eaf50c97d4e19d0a23d915e5d4e870af428ff693683146158fe3feeea5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
